{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Custom Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patterns for Adding Custom Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can go wrong and why is it important to get this right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Follow the sci-kit learn API: https://scikit-learn.org/stable/developers/develop.html#rolling-your-own-estimator\n",
    "- Favor array-like interface data structures internally in estimators. These work better across numpy, Dask, and Rapids that dataframe collections. If you need dataframe operations (like groupby, etc.) consider moving those to a transformer and converting to an array-like for processing.\n",
    "- All attributes learned during .fit should be concrete, i.e. they should not be dask collections.\n",
    "- To the extent possible, transformers should support\n",
    "        numpy.ndarray\n",
    "        pandas.DataFrame\n",
    "        dask.Array\n",
    "        dask.DataFrame\n",
    "- If possible, transformers should accept a columns keyword to limit the transformation to just those columns, while passing through other columns untouched. inverse_transform should behave similarly (ignoring other columns) so that inverse_transform(transform(X)) equals X.\n",
    "- Methods returning arrays (like .transform, .predict), should return the same type as the input. So if a dask.array is passed in, a dask.array with the same chunks should be returned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last 4 from: https://ml.dask.org/contributing.html#conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend `dask-ml` Using a Real World Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup/data\n",
    "- Simple custom transformer (maybe fillna or similar?)\n",
    "- transformer for adding columns (this will allow us to differentiate between dask and pandas)\n",
    "- custom estimator\n",
    "- make the previous steps into a pipeline (look at the beginning of the GTC talk and find the part where Mike talks about not running fit a second time. Might want todo something with that to show the value of a pipeline here.\n",
    "- Grid search - show running their pipeline with grid search to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "X_array, y_array = make_classification(\n",
    "    n_samples=10_000,\n",
    "    n_features=50,\n",
    "    weights=[0.75, 0.25],\n",
    "    flip_y=0.75,\n",
    "    random_state=123,\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(X_array, columns = [f\"var{i}\" for i in range(0,50)])\n",
    "y = pd.Series(y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.datasets import make_classification_df\n",
    "\n",
    "X_dask, y_dask = make_classification_df(\n",
    "    n_samples=10_000,\n",
    "    n_features=50,\n",
    "#     weights=[0.75, 0.25],\n",
    "#     flip_y=0.75,\n",
    "    random_state=123,\n",
    "    chunks=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer: Add Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_array, _ = make_classification(\n",
    "    n_samples=10_000,\n",
    "    n_features=1,\n",
    "    n_informative=1,\n",
    "    n_redundant=0,\n",
    "    n_classes=1,\n",
    "    random_state=123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "new_feature = pd.Series(np.random.randn(10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from abc import ABCMeta\n",
    "\n",
    "class AddFeature(TransformerMixin, BaseEstimator, metaclass=ABCMeta):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Add Feature to X\"\"\"\n",
    "        \n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from abc import ABCMeta\n",
    "import dask.dataframe as dd\n",
    "\n",
    "class AddFeature(TransformerMixin, BaseEstimator, metaclass=ABCMeta):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Add Feature to X\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X['var50'] = new_feature\n",
    "        elif isinstance(X, dd.DataFrame):\n",
    "            X['var50'] = dd.from_pandas(new_feature, npartitions=X.npartitions)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AF = AddFeature()\n",
    "X = AF.transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AF = AddFeature()\n",
    "X_dask = AF.transform(X_dask)\n",
    "X_dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('exp a')\n",
    "\n",
    "class CustomEstimator(BaseEstimator):    \n",
    "    def __init__(self, estimator, logger):\n",
    "        self.estimator = estimator        \n",
    "        self.logger = logger\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_kws):\n",
    "        self.estimator.fit(X, y)        \n",
    "        self.logger.info(\"... log things ...\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "clf = CustomEstimator(xgb.XGBClassifier(n_jobs=-1, eval_metric='error', use_label_encoder=False), logger)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.estimator.evals_result() # or something to show we trained the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Estimator with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit your custom estimator with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import xgboost.dask as dxgb\n",
    "\n",
    "clf = CustomEstimator(dxgb.DaskXGBClassifier(n_jobs=-1, eval_metric='error', use_label_encoder=False), logger)\n",
    "clf.fit(X_dask, y_dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
