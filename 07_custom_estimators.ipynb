{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Custom Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patterns for Adding Custom Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can go wrong and why is it important to get this right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "# Define a pipeline to search for the best combination of PCA truncation\n",
    "# and classifier regularization.\n",
    "pca = PCA()\n",
    "# set the tolerance to a large value to make the example faster\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'pca__n_components': [5, 15, 30, 45, 64],\n",
    "    'logistic__C': np.logspace(-4, 4, 4),\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=123)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best = search.best_estimator_\n",
    "\n",
    "print(f\"Training set score: {best.score(X_train, y_train)}\")\n",
    "print(f\"Test set score: {best.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(X):\n",
    "    \"\"\"Mutates X\"\"\"\n",
    "    # ... do something ...\n",
    "    return X\n",
    "\n",
    "pca = PCA(n_components=search.best_params_['pca__n_components'])\n",
    "logistic = LogisticRegression(\n",
    "    max_iter=10000, tol=0.1, C=search.best_params_['logistic__C'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_digits, y_digits, random_state=123)\n",
    "\n",
    "X_train = pca.fit_transform(X_train, y_train)\n",
    "X_train = mutate(X_train)\n",
    "logistic = logistic.fit(X_train, y_train)\n",
    "\n",
    "X_test = pca.transform(X_test) # <- Don't call fit again!\n",
    "X_test = mutate(X_test) # <-Don’t forget to call mutate on X_test!\n",
    "\n",
    "print(f\"Training set score: {logistic.score(X_train, y_train)}\")\n",
    "print(f\"Test set score: {logistic.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-18e5664ad406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'OR' is not defined"
     ]
    }
   ],
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PcaLogRegBlock:\n",
    "    def __init__(self, filepath, n_components, C):\n",
    "        self.filepath = filepath\n",
    "        self.n_components = n_components\n",
    "        self.C = C\n",
    "    def load_data(self):\n",
    "        self.df = pd.read_csv(self.filepath)\n",
    "    def add_features(self, X):\n",
    "        \"\"\"Fetches data from alternate source and merges into X.\n",
    "        \"\"\"\n",
    "        # ... mutate X ...\n",
    "        return X\n",
    "    def build_model(self):\n",
    "        X, y = self.df[self.features], self.df[self.target]\n",
    "        pca = PCA(n_components=self.n_components)\n",
    "        logistic = LogisticRegression(max_iter=10000, tol=0.1, C=self.C)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, random_state=123)\n",
    "        X_train = pca.fit_transform(X_train, y_train)\n",
    "        X_train = self.add_features(X_train)\n",
    "        return logistic.fit(X_train, y_train)\n",
    "    def run(self):\n",
    "        self.load_data()\n",
    "        model = self.build_model()\n",
    "        self.save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PcaLogRegGridSearchBlock:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.df = pd.read_csv(self.filepath)\n",
    "        self.best_params_ = {}\n",
    "    def run(self):\n",
    "        X, y = self.df[self.features], self.df[self.target]\n",
    "        param_grid = {\n",
    "            'n_components': [5, 15, 30, 45, 64],\n",
    "        }\n",
    "        search = GridSearchCV(PCA(), param_grid, n_jobs=-1)\n",
    "        search.fit(X, y)\n",
    "        self.best_params_.update(search.best_params_)\n",
    "        param_grid = {\n",
    "            'C': np.logspace(-4, 4, 4),\n",
    "        }\n",
    "        search = GridSearchCV(LogisticRegression(max_iter=10000, tol=0.1), param_grid, n_jobs=-1)\n",
    "        search.fit(X, y)\n",
    "        self.best_params_.update(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver():\n",
    "    gs = PcaLogRegGridSearchBlock('/path/to/file', ...)\n",
    "    gs.run()\n",
    "    # Use best results from grid search to retrain\n",
    "    pca_logreg = PcaLogRegBlock('/path/to/file', gs.best_params_['n_components'], gs.best_params_['C'])\n",
    "    pca_logreg.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Follow the sci-kit learn API: https://scikit-learn.org/stable/developers/develop.html#rolling-your-own-estimator\n",
    "- Favor array-like interface data structures internally in estimators. These work better across numpy, Dask, and Rapids that dataframe collections. If you need dataframe operations (like groupby, etc.) consider moving those to a transformer and converting to an array-like for processing.\n",
    "- All attributes learned during .fit should be concrete, i.e. they should not be dask collections.\n",
    "- To the extent possible, transformers should support\n",
    "        numpy.ndarray\n",
    "        pandas.DataFrame\n",
    "        dask.Array\n",
    "        dask.DataFrame\n",
    "- If possible, transformers should accept a columns keyword to limit the transformation to just those columns, while passing through other columns untouched. inverse_transform should behave similarly (ignoring other columns) so that inverse_transform(transform(X)) equals X.\n",
    "- Methods returning arrays (like .transform, .predict), should return the same type as the input. So if a dask.array is passed in, a dask.array with the same chunks should be returned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last 4 from: https://ml.dask.org/contributing.html#conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `dask-ml` to Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "X_array, y_array = make_classification(\n",
    "    n_samples=10_000,\n",
    "    n_features=50,\n",
    "    weights=[0.75, 0.25],\n",
    "    flip_y=0.75,\n",
    "    random_state=123,\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(X_array, columns = [f\"var{i}\" for i in range(0,50)])\n",
    "y = pd.Series(y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.datasets import make_classification_df\n",
    "\n",
    "X_dask, y_dask = make_classification_df(\n",
    "    n_samples=10_000,\n",
    "    n_features=50,\n",
    "#     weights=[0.75, 0.25],\n",
    "#     flip_y=0.75,\n",
    "    random_state=123,\n",
    "    chunks=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer: Add Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "new_feature = pd.Series(np.random.randn(10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from abc import ABCMeta\n",
    "\n",
    "class AddFeature(TransformerMixin, BaseEstimator, metaclass=ABCMeta):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Add Feature to X\"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from abc import ABCMeta\n",
    "import dask.dataframe as dd\n",
    "\n",
    "class AddFeature(TransformerMixin, BaseEstimator, metaclass=ABCMeta):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Add Feature to X\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X['var50'] = new_feature\n",
    "        elif isinstance(X, dd.DataFrame):\n",
    "            X['var50'] = dd.from_pandas(new_feature, npartitions=X.npartitions)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AF = AddFeature()\n",
    "X = AF.transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AF = AddFeature()\n",
    "X_dask = AF.transform(X_dask)\n",
    "X_dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('exp a')\n",
    "\n",
    "class CustomEstimator(BaseEstimator):    \n",
    "    def __init__(self, estimator, logger):\n",
    "        self.estimator = estimator        \n",
    "        self.logger = logger\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_kws):\n",
    "        self.estimator.fit(X, y)        \n",
    "        self.logger.info(\"... log things ...\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "clf = CustomEstimator(xgb.XGBClassifier(n_jobs=-1, eval_metric='error', use_label_encoder=False), logger)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.estimator.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Estimator with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit your custom estimator with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import xgboost.dask as dxgb\n",
    "\n",
    "clf = CustomEstimator(dxgb.DaskXGBClassifier(n_jobs=-1, eval_metric='error', use_label_encoder=False), logger)\n",
    "clf.fit(X_dask, y_dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the column we added from the previous transformation\n",
    "X = X.drop(columns=['var50'])\n",
    "X_dask = X_dask.drop(columns=['var50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm we've removed that column\n",
    "X_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm we've removed that column\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# your pipeline here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "custom_estimator = CustomEstimator(xgb.XGBClassifier(n_jobs=-1, eval_metric='error', use_label_encoder=False), logger)\n",
    "pipe = Pipeline(steps=[('add_feature', AddFeature()), ('custom_estimator', custom_estimator)])\n",
    "\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost.dask as dxgb\n",
    "\n",
    "custom_estimator_dask = CustomEstimator(dxgb.DaskXGBClassifier(n_jobs=-1, eval_metric='error', use_label_encoder=False), logger)\n",
    "pipe = Pipeline(steps=[('add_feature', AddFeature()), ('custom_estimator_dask', custom_estimator_dask)])\n",
    "\n",
    "pipe.fit(X_dask, y_dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm we added the feature\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm we added the feature\n",
    "X_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_estimator.estimator.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_estimator_dask.estimator.feature_importances_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
