{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Custom Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patterns for Adding Custom Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic `scikit-learn` pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "# Define a pipeline to search for the best combination of PCA truncation\n",
    "# and classifier regularization.\n",
    "pca = PCA()\n",
    "# set the tolerance to a large value to make the example faster\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'pca__n_components': [5, 15, 30, 45, 64],\n",
    "    'logistic__C': np.logspace(-4, 4, 4),\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=123)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best = search.best_estimator_\n",
    "\n",
    "print(f\"Training set score: {best.score(X_train, y_train)}\")\n",
    "print(f\"Test set score: {best.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make a change to our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(X):\n",
    "    \"\"\"Mutates X\"\"\"\n",
    "    # ... do something ...\n",
    "    return X\n",
    "\n",
    "pca = PCA(n_components=search.best_params_['pca__n_components'])\n",
    "logistic = LogisticRegression(\n",
    "    max_iter=10000, tol=0.1, C=search.best_params_['logistic__C'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_digits, y_digits, random_state=123)\n",
    "\n",
    "X_train = pca.fit_transform(X_train, y_train)\n",
    "X_train = mutate(X_train)\n",
    "logistic = logistic.fit(X_train, y_train)\n",
    "\n",
    "X_test = pca.transform(X_test) # <- Don't call fit again!\n",
    "X_test = mutate(X_test) # <-Don’t forget to call mutate on X_test!\n",
    "\n",
    "print(f\"Training set score: {logistic.score(X_train, y_train)}\")\n",
    "print(f\"Test set score: {logistic.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hm, maybe there is a better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PcaLogRegBlock:\n",
    "    def __init__(self, filepath, n_components, C):\n",
    "        self.filepath = filepath\n",
    "        self.n_components = n_components\n",
    "        self.C = C\n",
    "    def load_data(self):\n",
    "        self.df = pd.read_csv(self.filepath)\n",
    "    def mutate(self, X):\n",
    "        \"\"\"Fetches data from alternate source and merges into X.\n",
    "        \"\"\"\n",
    "        # ... mutate X ...\n",
    "        return X\n",
    "    def build_model(self):\n",
    "        X, y = self.df[self.features], self.df[self.target]\n",
    "        pca = PCA(n_components=self.n_components)\n",
    "        logistic = LogisticRegression(max_iter=10000, tol=0.1, C=self.C)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, random_state=123)\n",
    "        X_train = pca.fit_transform(X_train, y_train)\n",
    "        X_train = self.mutate(X_train)\n",
    "        return logistic.fit(X_train, y_train)\n",
    "    def run(self):\n",
    "        self.load_data()\n",
    "        model = self.build_model()\n",
    "        self.save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PcaLogRegGridSearchBlock:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.df = pd.read_csv(self.filepath)\n",
    "        self.best_params_ = {}\n",
    "    def run(self):\n",
    "        X, y = self.df[self.features], self.df[self.target]\n",
    "        param_grid = {\n",
    "            'n_components': [5, 15, 30, 45, 64],\n",
    "        }\n",
    "        search = GridSearchCV(PCA(), param_grid, n_jobs=-1)\n",
    "        search.fit(X, y)\n",
    "        self.best_params_.update(search.best_params_)\n",
    "        param_grid = {\n",
    "            'C': np.logspace(-4, 4, 4),\n",
    "        }\n",
    "        search = GridSearchCV(LogisticRegression(max_iter=10000, tol=0.1), param_grid, n_jobs=-1)\n",
    "        search.fit(X, y)\n",
    "        self.best_params_.update(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver():\n",
    "    gs = PcaLogRegGridSearchBlock('/path/to/file', ...)\n",
    "    gs.run()\n",
    "    # Use best results from grid search to retrain\n",
    "    pca_logreg = PcaLogRegBlock('/path/to/file', gs.best_params_['n_components'], gs.best_params_['C'])\n",
    "    pca_logreg.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Best Option for adding code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from abc import ABCMeta\n",
    "\n",
    "class Mutate(TransformerMixin, BaseEstimator, metaclass=ABCMeta):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Mutates X\"\"\"\n",
    "        # ... do something ...\n",
    "        return X\n",
    "\n",
    "pca = PCA(n_components=search.best_params_['pca__n_components'])\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1, C=search.best_params_['logistic__C'])\n",
    "\n",
    "pipe = Pipeline(steps=[('pca', pca), ('mutate', Mutate()), ('logistic', logistic)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=123)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print(f\"Training set score: {pipe.score(X_train, y_train)}\")\n",
    "print(f\"Test set score: {pipe.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Follow the sci-kit learn API: https://scikit-learn.org/stable/developers/develop.html#rolling-your-own-estimator\n",
    "- Favor array-like interface data structures internally in estimators. These work better across numpy, Dask, and Rapids that dataframe collections. If you need dataframe operations (like groupby, etc.) consider moving those to a transformer and converting to an array-like for processing.\n",
    "- All attributes learned during .fit should be concrete, i.e. they should not be dask collections.\n",
    "- To the extent possible, transformers should support\n",
    "        numpy.ndarray\n",
    "        pandas.DataFrame\n",
    "        dask.Array\n",
    "        dask.DataFrame\n",
    "- If possible, transformers should accept a columns keyword to limit the transformation to just those columns, while passing through other columns untouched. inverse_transform should behave similarly (ignoring other columns) so that inverse_transform(transform(X)) equals X.\n",
    "- Methods returning arrays (like .transform, .predict), should return the same type as the input. So if a dask.array is passed in, a dask.array with the same chunks should be returned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last 4 from: https://ml.dask.org/contributing.html#conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `dask-ml` to Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "X_array, y_array = make_classification(\n",
    "    n_samples=10_000,\n",
    "    n_features=50,\n",
    "    random_state=123,\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(X_array, columns = [f\"var{i}\" for i in range(0,50)])\n",
    "y = pd.Series(y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.datasets import make_classification_df\n",
    "\n",
    "X_dask, y_dask = make_classification_df(\n",
    "    n_samples=10_000,\n",
    "    n_features=50,\n",
    "    random_state=123,\n",
    "    chunks=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer: Add Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "new_feature = pd.Series(np.random.randn(10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from abc import ABCMeta\n",
    "\n",
    "class AddFeature(TransformerMixin, BaseEstimator, metaclass=ABCMeta):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Add Feature to X\"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from abc import ABCMeta\n",
    "import dask.dataframe as dd\n",
    "\n",
    "class AddFeature(TransformerMixin, BaseEstimator, metaclass=ABCMeta):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Add Feature to X\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X['var50'] = new_feature\n",
    "        elif isinstance(X, dd.DataFrame):\n",
    "            X['var50'] = dd.from_pandas(new_feature, npartitions=X.npartitions)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AF = AddFeature()\n",
    "X = AF.transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AF = AddFeature()\n",
    "X_dask = AF.transform(X_dask)\n",
    "X_dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('exp a')\n",
    "\n",
    "class CustomEstimator(BaseEstimator):    \n",
    "    def __init__(self, estimator, logger):\n",
    "        self.estimator = estimator        \n",
    "        self.logger = logger\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_kws):\n",
    "        self.estimator.fit(X, y)        \n",
    "        self.logger.info(\"... log things ...\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "clf = CustomEstimator(xgb.XGBClassifier(n_jobs=-1, eval_metric='error', use_label_encoder=False), logger)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.estimator.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Estimator with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit your custom estimator with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import xgboost.dask as dxgb\n",
    "\n",
    "clf = CustomEstimator(dxgb.DaskXGBClassifier(n_jobs=-1, eval_metric='error', use_label_encoder=False), logger)\n",
    "clf.fit(X_dask, y_dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.estimator.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the column we added from the previous transformation\n",
    "X = X.drop(columns=['var50'])\n",
    "X_dask = X_dask.drop(columns=['var50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm we've removed that column\n",
    "X_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm we've removed that column\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "custom_estimator = CustomEstimator(xgb.XGBClassifier(n_jobs=-1, eval_metric='error', use_label_encoder=False), logger)\n",
    "pipe = Pipeline(steps=[('add_feature', AddFeature()), ('custom_estimator', custom_estimator)])\n",
    "\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm we added the feature\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm we have a trained model\n",
    "custom_estimator.estimator.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# your dask pipeline here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost.dask as dxgb\n",
    "\n",
    "custom_estimator_dask = CustomEstimator(dxgb.DaskXGBClassifier(n_jobs=-1, eval_metric='error', use_label_encoder=False), logger)\n",
    "pipe_dask = Pipeline(steps=[('add_feature', AddFeature()), ('custom_estimator_dask', custom_estimator_dask)])\n",
    "\n",
    "pipe_dask.fit(X_dask, y_dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm we added the feature\n",
    "X_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm we have a trained model\n",
    "custom_estimator_dask.estimator.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an adaptation of the code in this article: https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from dateutil.relativedelta import *\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "class TimeBasedCV(BaseCrossValidator):\n",
    "    '''\n",
    "    Parameters \n",
    "    ----------\n",
    "    train_period: int\n",
    "        number of time units to include in each train set\n",
    "        default is 30\n",
    "    test_period: int\n",
    "        number of time units to include in each test set\n",
    "        default is 7\n",
    "    freq: string\n",
    "        frequency of input parameters. possible values are: days, months, years, weeks, hours, minutes, seconds\n",
    "        possible values designed to be used by dateutil.relativedelta class\n",
    "        deafault is days\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, train_period=30, test_period=7, freq='days'):\n",
    "        self.train_period = train_period\n",
    "        self.test_period = test_period\n",
    "        self.freq = freq\n",
    "        \n",
    "        \n",
    "    def split(self, data, validation_split_date=None, date_column='date', gap=0):\n",
    "        '''\n",
    "        Generate indices to split data into training and test set\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        data: pandas DataFrame\n",
    "            your data, contain one column for the record date \n",
    "        validation_split_date: datetime.date()\n",
    "            first date to perform the splitting on.\n",
    "            if not provided will set to be the minimum date in the data after the first training set\n",
    "        date_column: string, deafult='record_date'\n",
    "            date of each record\n",
    "        gap: int, default=0\n",
    "            for cases the test set does not come right after the train set,\n",
    "            *gap* days are left between train and test sets\n",
    "        \n",
    "        Returns \n",
    "        -------\n",
    "        train_index ,test_index: \n",
    "            list of tuples (train index, test index) similar to sklearn model selection\n",
    "        '''\n",
    "        \n",
    "        # check that date_column exist in the data:\n",
    "        try:\n",
    "            data[date_column]\n",
    "        except:\n",
    "            raise KeyError(date_column)\n",
    "            \n",
    "        train_indices_list = []\n",
    "        test_indices_list = []\n",
    "\n",
    "        if validation_split_date==None:\n",
    "            validation_split_date = data[date_column].min().date() + relativedelta(**{self.freq: self.train_period})\n",
    "        \n",
    "        start_train = validation_split_date - relativedelta(**{self.freq: self.train_period})\n",
    "        end_train = start_train + relativedelta(**{self.freq: self.train_period})\n",
    "        start_test = end_train + relativedelta(**{self.freq: gap})\n",
    "        end_test = start_test + relativedelta(**{self.freq: self.test_period})\n",
    "\n",
    "        while end_test < data[date_column].max().date():\n",
    "            # train indices:\n",
    "            cur_train_indices = list(data[(data[date_column].dt.date>=start_train) & \n",
    "                                     (data[date_column].dt.date<end_train)].index)\n",
    "\n",
    "            # test indices:\n",
    "            cur_test_indices = list(data[(data[date_column].dt.date>=start_test) &\n",
    "                                    (data[date_column].dt.date<end_test)].index)\n",
    "            \n",
    "            print(\"Train period:\",start_train,\"-\" , end_train, \", Test period\", start_test, \"-\", end_test,\n",
    "                  \"# train records\", len(cur_train_indices), \", # test records\", len(cur_test_indices))\n",
    "\n",
    "            train_indices_list.append(cur_train_indices)\n",
    "            test_indices_list.append(cur_test_indices)\n",
    "\n",
    "            # update dates:\n",
    "            \n",
    "            start_train = start_train + relativedelta(**{self.freq: self.test_period})\n",
    "            end_train = start_train + relativedelta(**{self.freq: self.train_period})\n",
    "            start_test = end_train + relativedelta(**{self.freq: gap})\n",
    "            end_test = start_test + relativedelta(**{self.freq: self.test_period})\n",
    "\n",
    "        # mimic sklearn output  \n",
    "        index_output = [(train,test) for train,test in zip(train_indices_list,test_indices_list)]\n",
    "\n",
    "        self.n_splits = len(index_output)\n",
    "        \n",
    "        return index_output\n",
    "    \n",
    "    \n",
    "    def get_n_splits(self):\n",
    "        \"\"\"Returns the number of splitting iterations in the cross-validator\n",
    "        Returns\n",
    "        -------\n",
    "        n_splits : int\n",
    "            Returns the number of splitting iterations in the cross-validator.\n",
    "        \"\"\"\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from dask_ml.datasets import random_date\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1_000,\n",
    "    n_features=5,\n",
    "    random_state=123,\n",
    ")\n",
    "dates = (date(2020, 1, 1), date(2021, 1, 1))\n",
    "columns = [\"var\" + str(i) for i in range(np.shape(X)[1])]\n",
    "y_series = pd.Series(y, name='target')\n",
    "X_df = pd.DataFrame(X, columns=columns)\n",
    "X_df['date'] = [random_date(*dates) for _ in range(len(X_df))]\n",
    "X_df['date'] = X_df['date'].astype('datetime64')\n",
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeBasedCV(train_period=90, test_period=30)\n",
    "index_output = tscv.split(X_df, validation_split_date=date(2020,6,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'custom_estimator__estimator__min_child_weight': [1, 5, 10],\n",
    "    # Leaving these out to run faster for testing\n",
    "    # 'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    # 'subsample': [0.6, 0.8, 1.0],\n",
    "    # 'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    # 'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = pipe,\n",
    "    n_jobs = -1,\n",
    "    param_grid = params,\n",
    "    cv = index_output,\n",
    "    scoring='roc_auc',\n",
    "    verbose=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search.fit(X_df.drop('date', axis=1), y_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.datasets import random_date\n",
    "from datetime import date\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask_ml.datasets import make_classification_df\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_classification_df(\n",
    "    n_samples=1_00,\n",
    "    n_features=5,\n",
    "    random_state=123,\n",
    "    chunks=1000\n",
    ")\n",
    "\n",
    "columns = [\"var\" + str(i) for i in range(np.shape(X)[1])]\n",
    "dates_col = pd.Series([random_date(*(date(2020, 1, 1), date(2021, 1, 1))) for _ in range(len(X))])\n",
    "X['date'] = dates_col\n",
    "X['date'] = X['date'].astype('datetime64')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeBasedCV(train_period=90, test_period=30)\n",
    "index_output = tscv.split(X, validation_split_date=date(2020,6,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'custom_estimator_dask__estimator__min_child_weight': [1, 5, 10],\n",
    "    # Leaving these out to run faster for testing\n",
    "    # 'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    # 'subsample': [0.6, 0.8, 1.0],\n",
    "    # 'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    # 'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = pipe_dask,\n",
    "    n_jobs = -1,\n",
    "    param_grid = params,\n",
    "    cv = index_output,\n",
    "    scoring='roc_auc',\n",
    "    verbose=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search.fit(X.drop('date', axis=1), y_series)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
