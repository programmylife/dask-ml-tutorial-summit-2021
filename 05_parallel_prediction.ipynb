{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score and Predict Large Datasets\n",
    "================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll train on a smaller dataset that fits in memory, but need to predict or score for a much larger (possibly larger than memory) dataset. Perhaps your [learning curve](http://scikit-learn.org/stable/modules/learning_curve.html) has leveled off, or you only have labels for a subset of the data.\n",
    "\n",
    "In this situation, you can use [ParallelPostFit](http://ml.dask.org/modules/generated/dask_ml.wrappers.ParallelPostFit.html) to parallelize and distribute the scoring or prediction steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "\n",
    "client = Client(\n",
    "    processes=False,\n",
    "    threads_per_worker=4,\n",
    "    n_workers=1,\n",
    "    memory_limit=\"2GB\",\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generate a small random dataset with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "X_train, y_train = make_classification(\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    random_state=1,\n",
    "    n_clusters_per_class=1,\n",
    "    n_samples=1000,\n",
    ")\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll clone that dataset many times with `dask.array`. `X_large` and `y_large` represent our larger than memory dataset. To scale up, increase N, the number of times we replicate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "X_large = da.concatenate(\n",
    "    [da.from_array(X_train, chunks=X_train.shape) for _ in range(N)]\n",
    ")\n",
    "y_large = da.concatenate(\n",
    "    [da.from_array(y_train, chunks=y_train.shape) for _ in range(N)]\n",
    ")\n",
    "X_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our training dataset fits in memory, we can use a scikit-learn estimator as the actual estimator fit during training.\n",
    "But we know that we'll want to predict for a large dataset, so we'll wrap the scikit-learn estimator with `ParallelPostFit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "\n",
    "\n",
    "clf = ParallelPostFit(LogisticRegressionCV(cv=3), scoring=\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the note in the `dask-ml`'s documentation about when and why a `scoring` parameter is needed.\n",
    "> https://ml.dask.org/modules/generated/dask_ml.wrappers.ParallelPostFit.html#dask_ml.wrappers.ParallelPostFit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit the dataset. Dask-ML does nothing here - we'll use the original `X_train` and `y_train` since this step can only use datasets that fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that training is done, we'll turn to Dask-ML for predicting on the full (larger than memory) dataset.\n",
    "\n",
    "We'll leave this as a quick exercise. Just like the previous step, we can predict with the `ParallelPostFit` using the same, familiar Scikit-learn function calls. Just make sure to use `X_large` this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the `ParallelPostFit` classifier `clf` to predict with the `X_large` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_large)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of `clf.predict` is Dask arary. Up to this point, we still haven't done any actual computation on our large dataset. Workers can write the predicted values to a shared file system, without ever having to collect the data on a single machine.\n",
    "\n",
    "We can also check the model's score on the entire large dataset. The computation will be done in parallel, and no single machine will have to hold all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_large, y_large)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
